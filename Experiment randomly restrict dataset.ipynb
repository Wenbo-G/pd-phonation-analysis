{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an auxiliary experiment to show that the decrease in performance are not due to a reduction in training and test size. \n",
    "\n",
    "We do so by randomly reducing the size of the data sets to the same number as our modified evaluation method, and with the same models, that is, until the OPD data has 96 samples, and until the mPower data has ~18300 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dataHandler as dh\n",
    "from os.path import join\n",
    "\n",
    "OPD_samples = pd.read_csv(join('Data','OPD_data.csv'))\n",
    "OPD_participants = pd.read_csv(join('Data','OPD_participants.csv'))\n",
    "\n",
    "\n",
    "print('One of the participants (S31) does not have demographic data (i.e., age and gender), which is why their submissions are missing from counts that include anything to do with gender or age\\n')\n",
    "\n",
    "dh.OPD_summary(OPD_samples, OPD_participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataLoader as dl\n",
    "import mPower_data_adjustments as mda\n",
    "\n",
    "\n",
    "#load the original submission data, the features extracted from them, and information needed to filter them\n",
    "submissions = dl.pickleLoad(join('Data','submissions_Full.pickle'))\n",
    "features = dl.pickleLoad(join('Data','combinedFeatures.pickle'))\n",
    "d2 = dl.pickleLoad(join('Data','d2Feature5.pickle'))\n",
    "rms_energy = dl.pickleLoad(join('Data','rms_energy_notNormalised.pickle'))\n",
    "\n",
    "features.rename(columns={'recordID':'recordId'},inplace=True)\n",
    "d2 = d2[d2['d2'] != 'nan'] #drop nans in d2\n",
    "\n",
    "#filter out bad submissions and see how many remains\n",
    "rms_energy = rms_energy[rms_energy['rms_1_mean'] > 300] #250\n",
    "rms_energy = rms_energy[rms_energy['rms_1_std'] < 2000] #2500\n",
    "rms_energy = rms_energy[rms_energy['energy_1_mean'] > 50000] #45000\n",
    "features = features[features['Degree of voice breaks (%)'] < 30] #40\n",
    "features = features[features['Fraction of locally unvoiced frames (%)'] < 30]#40\n",
    "\n",
    "#combine them and see how many remains\n",
    "mPower_samples = pd.merge(rms_energy['recordId'],submissions,on='recordId')\n",
    "mPower_samples = pd.merge(mPower_samples,features,on='recordId')\n",
    "mPower_samples = mPower_samples.merge(d2[['recordId','d2']],on='recordId',how='inner')\n",
    "mPower_samples = mPower_samples[~(mPower_samples['gender'] == 'Prefer not to answer')] #retains only male and female\n",
    "\n",
    "#Remove samples that I found were bad, by looking at the 10 most extreme values for all 22 features and listening\n",
    "mPower_samples = mda.remove_bad_samples(mPower_samples)\n",
    "\n",
    "#Recalculate spread1 and spread2, the original was wrong.\n",
    "mPower_samples = mda.recalculate_spreads(mPower_samples)\n",
    "\n",
    "#Adjust the scale of several mPower features to be on the same scale as OPD. \n",
    "#Things like Jitter (%) is currently 2%, but in OPD features would be 0.02\n",
    "mPower_samples = mda.adjust_feature_scale(mPower_samples)\n",
    "\n",
    "mPower_participants = mPower_samples.drop_duplicates(subset=['healthCode'])[['healthCode', 'age', 'diagnosis-year','gender','onset-year','professional-diagnosis']]\n",
    "\n",
    "#print summary of the dataset\n",
    "dh.mPower_summary(mPower_samples, mPower_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and evaluate the models (UNMODIFIED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ozkan_model import ozkan\n",
    "from caliskan_model import caliskan\n",
    "from ulhaq_model import ulhaq\n",
    "\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "import torch.nn as nn\n",
    "from datetime import date\n",
    "\n",
    "import evaluation\n",
    "import resultsHandler as rh\n",
    "\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_results_df():\n",
    "    ozkan_kf_results = rh.results_DataFrame()\n",
    "    caliskan_kf_results = rh.results_DataFrame()\n",
    "    ulhaq_kf_results = rh.results_DataFrame()\n",
    "    kf_results = {'ozkan':ozkan_kf_results, 'caliskan':caliskan_kf_results, 'ulhaq':ulhaq_kf_results}\n",
    "\n",
    "    ozkan_split_results = rh.multiple_runs_DataFrame()\n",
    "    caliskan_split_results = rh.multiple_runs_DataFrame()\n",
    "    ulhaq_split_results = rh.multiple_runs_DataFrame()\n",
    "    split_results = {'ozkan':ozkan_split_results, 'caliskan':caliskan_split_results, 'ulhaq':ulhaq_split_results}\n",
    "    \n",
    "    return kf_results,split_results\n",
    "    \n",
    "    \n",
    "def process_results(kf_results_dict,kfold_results,split_results_dict,traintest_results):\n",
    "    \n",
    "    ozkan_result,caliskan_result,ulhaq_result = rh.separate_results(kfold_results)\n",
    "    kf_results_dict['ozkan'].loc[rep] = ozkan_result['run results'].loc[0]\n",
    "    kf_results_dict['caliskan'].loc[rep] = caliskan_result['run results'].loc[0]\n",
    "    kf_results_dict['ulhaq'].loc[rep] = ulhaq_result['run results'].loc[0]\n",
    "    \n",
    "    ozkan_result,caliskan_result,ulhaq_result = rh.separate_results(traintest_results)\n",
    "    split_results_dict['ozkan'].loc[rep] = ozkan_result['run results'].loc[0]\n",
    "    split_results_dict['caliskan'].loc[rep] = caliskan_result['run results'].loc[0]\n",
    "    split_results_dict['ulhaq'].loc[rep] = ulhaq_result['run results'].loc[0]\n",
    "    \n",
    "    return kf_results_dict,split_results_dict\n",
    "\n",
    "\n",
    "def process_repeated_results(kf_results_dict,split_results_dict):\n",
    "    repeated_kf_results = rh.results_DataFrame()\n",
    "    repeated_split_results = rh.results_DataFrame()\n",
    "    \n",
    "    repeated_kf_results.loc['ozkan'] = rh.process_multiple_kfold_runs(kf_results_dict['ozkan'])\n",
    "    repeated_kf_results.loc['caliskan'] = rh.process_multiple_kfold_runs(kf_results_dict['caliskan'])\n",
    "    repeated_kf_results.loc['ulhaq'] = rh.process_multiple_kfold_runs(kf_results_dict['ulhaq'])\n",
    "\n",
    "    repeated_split_results.loc['ozkan'] = rh.process_multiple_runs(split_results_dict['ozkan'])\n",
    "    repeated_split_results.loc['caliskan'] = rh.process_multiple_runs(split_results_dict['caliskan'])\n",
    "    repeated_split_results.loc['ulhaq'] = rh.process_multiple_runs(split_results_dict['ulhaq'])\n",
    "    \n",
    "    return repeated_kf_results,repeated_split_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with OPD\n",
    "Using the best models found from previous grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfold_splits = 8\n",
    "repetitions = 30\n",
    "\n",
    "\n",
    "###################### Seeds ######################\n",
    "seeds = list(range(repetitions))\n",
    "###################################################\n",
    "\n",
    "######################## to_numpy method ########################\n",
    "def to_numpy(OPD_samples):\n",
    "    \n",
    "    features = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP', \n",
    "                'MDVP:PPQ', 'Jitter:DDP', 'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', \n",
    "                'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'spread1', 'spread2', 'D2', 'PPE']\n",
    "    \n",
    "    X = OPD_samples[features].to_numpy()\n",
    "    y = OPD_samples['status'].to_numpy()\n",
    "    \n",
    "    return X, y\n",
    "#################################################################\n",
    "\n",
    "######################## Global settings ########################\n",
    "preprocessors = [MinMaxScaler]\n",
    "preprocessing_methods = ['X only']\n",
    "global_settings = list(itertools.product(preprocessors,preprocessing_methods))\n",
    "##################################################################\n",
    "\n",
    "######################## Ozkan settings ########################\n",
    "components = [14]\n",
    "ks = [1]\n",
    "ozkan_settings = list(itertools.product(components,ks))\n",
    "################################################################\n",
    "\n",
    "######################## Caliskan settings ########################\n",
    "lrses = [[0.003]*4]\n",
    "epochses = [[400]*4]\n",
    "rhoses = [[0.15,0.25],]\n",
    "lamses = [[0.03,0.03],]\n",
    "Bses = [[2,2],]\n",
    "activationses = [[nn.ReLU,nn.Sigmoid]]\n",
    "latent_sizes = [6,]\n",
    "caliskan_settings = list(itertools.product(lrses,epochses,rhoses,lamses,Bses,activationses,latent_sizes))\n",
    "###################################################################\n",
    "\n",
    "######################## Ul-Haq settings ########################\n",
    "kernels = ['rbf']\n",
    "gammas = [0.2]\n",
    "Cs = [5]\n",
    "num_featureses = [14] #best at 10 in paper\n",
    "ulhaq_settings = list(itertools.product(kernels,gammas,Cs,num_featureses))\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_results_dict_mm,split_results_dict_mm = initialise_results_df()\n",
    "    \n",
    "for rep,seed in zip(range(repetitions),seeds):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    #Let us randomly remove samples until we are at the same point\n",
    "    OPD_samples_reduced = OPD_samples.sample(96)\n",
    "    cond = OPD_participants['participant'].isin(OPD_samples_reduced['participant'])\n",
    "    OPD_participants_reduced = OPD_participants[cond]\n",
    "\n",
    "    kfold_results,traintest_results = evaluation.unmodified('OPD',OPD_samples_reduced,OPD_participants_reduced,\n",
    "                                              to_numpy,global_settings,ozkan_settings,caliskan_settings,ulhaq_settings,\n",
    "                                              ozkan_method=ozkan,caliskan_method=caliskan,ulhaq_method=ulhaq,\n",
    "                                              seeds=[seed],repetitions=1,verbose_odds=0,#0.025\n",
    "                                              n_splits=kfold_splits,training_split=0.7)\n",
    "\n",
    "    kf_results_dict_mm,split_results_dict_mm = process_results(kf_results_dict_mm,kfold_results,split_results_dict_mm,traintest_results)\n",
    "    \n",
    "repeated_kf_results_mm,repeated_split_results_mm = process_repeated_results(kf_results_dict_mm,split_results_dict_mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(repeated_kf_results_mm)\n",
    "display(repeated_split_results_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Couldnt be bothered to write the code to make ulhaq use standard scaler and ozkan and caliskan use minmax, so I computed both and combined them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 8\n",
    "repetitions = 30\n",
    "\n",
    "\n",
    "###################### Seeds ######################\n",
    "seeds = list(range(repetitions))\n",
    "###################################################\n",
    "\n",
    "######################## to_numpy method ########################\n",
    "def to_numpy(OPD_samples):\n",
    "    \n",
    "    features = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP', \n",
    "                'MDVP:PPQ', 'Jitter:DDP', 'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', \n",
    "                'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'spread1', 'spread2', 'D2', 'PPE']\n",
    "    \n",
    "    X = OPD_samples[features].to_numpy()\n",
    "    y = OPD_samples['status'].to_numpy()\n",
    "    \n",
    "    return X, y\n",
    "#################################################################\n",
    "\n",
    "######################## Global settings ########################\n",
    "preprocessors = [StandardScaler]\n",
    "preprocessing_methods = ['X only']\n",
    "global_settings = list(itertools.product(preprocessors,preprocessing_methods))\n",
    "##################################################################\n",
    "\n",
    "######################## Ozkan settings ########################\n",
    "components = [14]\n",
    "ks = [1]\n",
    "ozkan_settings = list(itertools.product(components,ks))\n",
    "################################################################\n",
    "\n",
    "######################## Caliskan settings ########################\n",
    "lrses = [[0.003]*4]\n",
    "epochses = [[400]*4]\n",
    "rhoses = [[0.15,0.25],]\n",
    "lamses = [[0.03,0.03],]\n",
    "Bses = [[2,2],]\n",
    "activationses = [[nn.ReLU,nn.Sigmoid]]\n",
    "latent_sizes = [6,]\n",
    "caliskan_settings = list(itertools.product(lrses,epochses,rhoses,lamses,Bses,activationses,latent_sizes))\n",
    "###################################################################\n",
    "\n",
    "######################## Ul-Haq settings ########################\n",
    "kernels = ['rbf']\n",
    "gammas = [0.2]\n",
    "Cs = [5]\n",
    "num_featureses = [14] #best at 10 in paper\n",
    "ulhaq_settings = list(itertools.product(kernels,gammas,Cs,num_featureses))\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_results_dict_ss,split_results_dict_ss = initialise_results_df()\n",
    "    \n",
    "for rep,seed in zip(range(repetitions),seeds):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    #Let us randomly remove samples until we are at the same point\n",
    "    OPD_samples_reduced = OPD_samples.sample(96)\n",
    "    cond = OPD_participants['participant'].isin(OPD_samples_reduced['participant'])\n",
    "    OPD_participants_reduced = OPD_participants[cond]\n",
    "\n",
    "    kfold_results,traintest_results = evaluation.unmodified('OPD',OPD_samples_reduced,OPD_participants_reduced,\n",
    "                                              to_numpy,global_settings,ozkan_settings,caliskan_settings,ulhaq_settings,\n",
    "                                              ozkan_method=ozkan,caliskan_method=caliskan,ulhaq_method=ulhaq,\n",
    "                                              seeds=[seed],repetitions=1,verbose_odds=0,#0.025\n",
    "                                              n_splits=kfold_splits,training_split=0.7)\n",
    "\n",
    "    kf_results_dict_ss,split_results_dict_ss = process_results(kf_results_dict_ss,kfold_results,split_results_dict_ss,traintest_results)\n",
    "    \n",
    "repeated_kf_results_ss,repeated_split_results_ss = process_repeated_results(kf_results_dict_ss,split_results_dict_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(repeated_kf_results_ss)\n",
    "display(repeated_split_results_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine\n",
    "import copy\n",
    "\n",
    "repeated_kf_results = copy.copy(repeated_kf_results_mm)\n",
    "repeated_split_results = copy.copy(repeated_split_results_mm)\n",
    "\n",
    "repeated_kf_results.loc['ulhaq'] = repeated_kf_results_ss.loc['ulhaq']\n",
    "repeated_split_results.loc['ulhaq'] = repeated_split_results_ss.loc['ulhaq']\n",
    "\n",
    "display(repeated_kf_results)\n",
    "display(repeated_split_results)\n",
    "\n",
    "\n",
    "dl.pickleSave(repeated_kf_results,join('Results','repeated_kfold restricted OPD 27Jan22.pickle'))\n",
    "dl.pickleSave(repeated_split_results,join('Results','repeated_traintest restricted OPD 27Jan22.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with mPower\n",
    "Using the best models found from previous grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kfold_splits = 5\n",
    "repetitions = 10\n",
    "\n",
    "###################### Seeds ######################\n",
    "seeds = list(range(repetitions))\n",
    "###################################################\n",
    "\n",
    "######################## to_numpy method ########################\n",
    "\n",
    "def to_numpy(mPower_samples):\n",
    "\n",
    "    features = ['Mean pitch (Hz)','Minimum pitch (Hz)','Maximum pitch (Hz)','Jitter (local) (%)','Jitter (local, absolute)','Jitter (rap) (%)','Jitter (ppq5) (%)','Jitter (ddp) (%)',\n",
    "                'Shimmer (local) (%)','Shimmer (local, dB) (dB)','Shimmer (apq3) (%)','Shimmer (apq5) (%)','Shimmer (apq11) (%)','Shimmer (dda) (%)',\n",
    "                'Mean noise-to-harmonics ratio','Mean harmonics-to-noise ratio (dB)','spread1 (negative entropy of F0)','spread2 (standard error of F0)','PPE','DFA','RPDE','d2']\n",
    "    \n",
    "    X = mPower_samples[features].to_numpy()\n",
    "    y = (mPower_samples['professional-diagnosis']*1).to_numpy(dtype='int64')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "#The model we will be using for Ozkan is \"ozkan PCA_16 k_11 StandardScaler X only\"\n",
    "#This was the best model for both 10fold CV and 70/30 split\n",
    "\n",
    "#The model we will be using for Caliskan is \"caliskan ReLU Sigmoid latent:6, epochs:50, lr:0.0030 StandardScaler X only\"\n",
    "#This was the best model for both 10fold CV and 70/30 split\n",
    "\n",
    "#The model we will be using for Ulhaq is \"ulhaq rbf gamma:scale C:10 num_features:20 StandardScaler X only\"\n",
    "#This was the 2nd best model for 10fold CV and best for 70/30 split\n",
    "\n",
    "\n",
    "######################## Global settings ########################\n",
    "preprocessors = [StandardScaler]\n",
    "preprocessing_methods = ['X only']\n",
    "global_settings = list(itertools.product(preprocessors,preprocessing_methods))\n",
    "##################################################################\n",
    "\n",
    "######################## Ozkan settings ########################\n",
    "components = [16]\n",
    "ks = [11]\n",
    "ozkan_settings = list(itertools.product(components,ks))\n",
    "################################################################\n",
    "\n",
    "######################## Caliskan settings ########################\n",
    "lrses = [[0.003]*4,]\n",
    "epochses = [[50]*4,]\n",
    "rhoses = [[0.15,0.25],]\n",
    "lamses = [[0.03,0.03],]\n",
    "Bses = [[2,2],]\n",
    "activationses = [[nn.ReLU,nn.Sigmoid],]\n",
    "latent_sizes = [6,]\n",
    "caliskan_settings = list(itertools.product(lrses,epochses,rhoses,lamses,Bses,activationses,latent_sizes))\n",
    "###################################################################\n",
    "\n",
    "######################## Ul-Haq settings ########################\n",
    "kernels = ['rbf']\n",
    "gammas = ['scale']\n",
    "Cs = [10]\n",
    "num_featureses = [20] #best at 10 in paper\n",
    "ulhaq_settings = list(itertools.product(kernels,gammas,Cs,num_featureses))\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_results_dict,split_results_dict = initialise_results_df()\n",
    "    \n",
    "for rep,seed in zip(range(repetitions),seeds):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    #Let us randomly remove samples until we are at the same point\n",
    "    mPower_samples_reduced = mPower_samples.sample(n=18300)\n",
    "    mPower_participants_reduced = mPower_samples_reduced.drop_duplicates(subset=['healthCode'])[['healthCode', 'age', 'diagnosis-year','gender','onset-year','professional-diagnosis']]\n",
    "\n",
    "    kfold_results,traintest_results = evaluation.unmodified('mPower',mPower_samples_reduced,mPower_participants_reduced,\n",
    "                                             to_numpy,global_settings,ozkan_settings,caliskan_settings,ulhaq_settings,\n",
    "                                             ozkan_method=ozkan,caliskan_method=caliskan,ulhaq_method=ulhaq,\n",
    "                                             seeds=[seed],repetitions=1,verbose_odds=0,#0.025\n",
    "                                             n_splits=kfold_splits,training_split=0.7)\n",
    "\n",
    "    kf_results_dict,split_results_dict = process_results(kf_results_dict,kfold_results,split_results_dict,traintest_results)\n",
    "    \n",
    "repeated_kf_results,repeated_split_results = process_repeated_results(kf_results_dict,split_results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(repeated_kf_results)\n",
    "display(repeated_split_results)\n",
    "\n",
    "\n",
    "dl.pickleSave(repeated_kf_results,join('Results','repeated_kfold restricted mPower 26Jan22.pickle'))\n",
    "dl.pickleSave(repeated_split_results,join('Results','repeated_traintest restricted mPower 26Jan22.pickle'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that although there is a decrease in performance, it is tiny compare to the decrease from all the modifications. Thus we can conclude that the performance decrease is not because of a reduction of training and test samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PD Voice Replication 2)\n",
   "language": "python",
   "name": "pd_voice_replication_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
