{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This contains the methods and models for the paper \"Has machine learning over-promised in healthcare?\"\n",
    "\n",
    "It contains the models: \n",
    "- ozkan (Model A in the paper): PCA with nearest neighbours https://doi.org/10.3390/e18040115\n",
    "- caliskan (Model B in the paper): stacked auto encoder https://electricajournal.org/Content/files/sayilar/58/3311-3318.pdf\n",
    "- ulhaq (Model C in the paper): SVM with feature selection https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8672565"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to look at the each inflatory effect that was discussed in the manuscript, breaking down how much inflation to the performance it causes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prepare/preprocess data\n",
    "OPD is the Oxford Parkinsons Disease dataset, retrieved from UCI. This was originally created by Max Little, and is used extensively in PD classification research (https://archive.ics.uci.edu/ml/datasets/parkinsons)\n",
    "\n",
    "mPower is the larger dataset, from the mPower study, a Parkinsons mobile application developed by Sage Bionetworks and described in Synpase (doi:10.7303/syn4993293)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dataHandler as dh\n",
    "from os.path import join\n",
    "\n",
    "OPD_samples = pd.read_csv(join('Data','OPD_data.csv'))\n",
    "OPD_participants = pd.read_csv(join('Data','OPD_participants.csv'))\n",
    "\n",
    "print('One of the participants (S31) does not have demographic data (i.e., age and gender), which is why their submissions are missing from counts that include anything to do with gender or age\\n')\n",
    "\n",
    "dh.OPD_summary(OPD_samples, OPD_participants)\n",
    "\n",
    "#Include the S31 patient here because we dont use age or sex, so it is fine to include this person for this experiment\n",
    "OPD_participants.at[31,['participant','status']] = ('S31',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataLoader as dl\n",
    "import mPower_data_adjustments as mda\n",
    "\n",
    "#load the original submission data, the features extracted from them, and information needed to filter them\n",
    "submissions = dl.pickleLoad(join('Data','submissions_Full.pickle'))\n",
    "features = dl.pickleLoad(join('Data','combinedFeatures.pickle'))\n",
    "d2 = dl.pickleLoad(join('Data','d2Feature5.pickle'))\n",
    "rms_energy = dl.pickleLoad(join('Data','rms_energy_notNormalised.pickle'))\n",
    "\n",
    "features.rename(columns={'recordID':'recordId'},inplace=True)\n",
    "d2 = d2[d2['d2'] != 'nan'] #drop nans in d2\n",
    "\n",
    "#filter out bad submissions and see how many remains\n",
    "rms_energy = rms_energy[rms_energy['rms_1_mean'] > 300] #250\n",
    "rms_energy = rms_energy[rms_energy['rms_1_std'] < 2000] #2500\n",
    "rms_energy = rms_energy[rms_energy['energy_1_mean'] > 50000] #45000\n",
    "features = features[features['Degree of voice breaks (%)'] < 30] #40\n",
    "features = features[features['Fraction of locally unvoiced frames (%)'] < 30]#40\n",
    "\n",
    "#combine them and see how many remains\n",
    "mPower_samples = pd.merge(rms_energy['recordId'],submissions,on='recordId')\n",
    "mPower_samples = pd.merge(mPower_samples,features,on='recordId')\n",
    "mPower_samples = mPower_samples.merge(d2[['recordId','d2']],on='recordId',how='inner')\n",
    "mPower_samples = mPower_samples[~(mPower_samples['gender'] == 'Prefer not to answer')] #retains only male and female\n",
    "\n",
    "#Remove samples that I found were bad, by looking at the 10 most extreme values for all 22 features and listening\n",
    "mPower_samples = mda.remove_bad_samples(mPower_samples)\n",
    "\n",
    "#Recalculate spread1 and spread2, the original was wrong.\n",
    "mPower_samples = mda.recalculate_spreads(mPower_samples)\n",
    "\n",
    "#Adjust the scale of several mPower features to be on the same scale as OPD. \n",
    "#Things like Jitter (%) is currently 2%, but in OPD features would be 0.02\n",
    "mPower_samples = mda.adjust_feature_scale(mPower_samples)\n",
    "\n",
    "mPower_samples.index = range(len(mPower_samples))\n",
    "mPower_participants = mPower_samples.drop_duplicates(subset=['healthCode'])[['healthCode', 'age', 'diagnosis-year','gender','onset-year','professional-diagnosis']]\n",
    "\n",
    "#print summary of the dataset\n",
    "dh.mPower_summary(mPower_samples, mPower_participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ozkan_model import ozkan\n",
    "from caliskan_model import caliskan\n",
    "from ulhaq_model import ulhaq\n",
    "\n",
    "import itertools\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "import torch.nn as nn\n",
    "from datetime import date\n",
    "from os.path import join\n",
    "\n",
    "import evaluation\n",
    "import resultsHandler as rh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OPD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 8\n",
    "training_split=0.75 #its meant to be 0.7, but 0.75 results in clean proportions with participants (there is no integer value for 70% of 8 participants)\n",
    "repetitions = 30\n",
    "#repetitions = 3\n",
    "\n",
    "###################### Seeds ######################\n",
    "seeds = list(range(repetitions))\n",
    "###################################################\n",
    "\n",
    "\n",
    "######################## to_numpy method ########################\n",
    "\n",
    "def to_numpy(OPD_samples):\n",
    "    \n",
    "    features = ['MDVP:Fo(Hz)', 'MDVP:Fhi(Hz)', 'MDVP:Flo(Hz)', 'MDVP:Jitter(%)', 'MDVP:Jitter(Abs)', 'MDVP:RAP', \n",
    "                'MDVP:PPQ', 'Jitter:DDP', 'MDVP:Shimmer', 'MDVP:Shimmer(dB)', 'Shimmer:APQ3', 'Shimmer:APQ5', \n",
    "                'MDVP:APQ', 'Shimmer:DDA', 'NHR', 'HNR', 'RPDE', 'DFA', 'spread1', 'spread2', 'D2', 'PPE']\n",
    "    \n",
    "    X = OPD_samples[features].to_numpy()\n",
    "    y = OPD_samples['status'].to_numpy()\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "#The model we will be using for Ozkan is \"ozkan PCA_14 k_1 MinMaxScaler X only\"\n",
    "#This was the best model for 10fold CV, and best for 70/30 split\n",
    "\n",
    "#The model we will be using for Caliskan is \"caliskan ReLU Sigmoid latent:6, epochs:400, lr:0.0030 MinMaxScaler X only\"\n",
    "#This was the 2nd best model for 10fold CV, and best for 70/30 split\n",
    "\n",
    "#The model we will be using for Ul-Haq is \"ulhaq rbf gamma:0.2000 C:5 num_features:14 StandardScaler X only\"\n",
    "#This was the 2nd best model for both 10fold CV and 70/30 split\n",
    "\n",
    "######################## Global settings ########################\n",
    "preprocessors = [MinMaxScaler,StandardScaler]\n",
    "preprocessing_methods = ['X only']\n",
    "global_settings = list(itertools.product(preprocessors,preprocessing_methods))\n",
    "##################################################################\n",
    "\n",
    "######################## Ozkan settings ########################\n",
    "components = [14]\n",
    "ks = [1]\n",
    "ozkan_settings = list(itertools.product(components,ks))\n",
    "################################################################\n",
    "\n",
    "######################## Caliskan settings ########################\n",
    "lrses = [[0.003]*4]\n",
    "epochses = [[400]*4]\n",
    "rhoses = [[0.15,0.25],]\n",
    "lamses = [[0.03,0.03],]\n",
    "Bses = [[2,2],]\n",
    "activationses = [[nn.ReLU,nn.Sigmoid]]\n",
    "latent_sizes = [6,]\n",
    "caliskan_settings = list(itertools.product(lrses,epochses,rhoses,lamses,Bses,activationses,latent_sizes))\n",
    "###################################################################\n",
    "\n",
    "######################## Ul-Haq settings ########################\n",
    "kernels = ['rbf']\n",
    "gammas = [0.2]\n",
    "Cs = [5]\n",
    "num_featureses = [14] #best at 10 in paper\n",
    "ulhaq_settings = list(itertools.product(kernels,gammas,Cs,num_featureses))\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First the unmodified results (no inflatory effects removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kf_results_OPD_0 = dl.pickleLoad(join('Results','repeated_kfold with OPD unmodified 25October21.pickle'))\n",
    "split_results_OPD_0 = dl.pickleLoad(join('Results','repeated_traintest with OPD unmodified25October21.pickle'))\n",
    "\n",
    "rh.print_results(kf_results_OPD_0,top=3)\n",
    "rh.print_results(split_results_OPD_0,top=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant independence (removing the digital fingerprinting effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "samples=OPD_samples\n",
    "participants=OPD_participants\n",
    "\n",
    "ozkan_method=ozkan\n",
    "caliskan_method=caliskan\n",
    "ulhaq_method=ulhaq\n",
    "\n",
    "seeds=seeds\n",
    "n_splits=kfold_splits\n",
    "repetitions=repetitions\n",
    "verbose_odds=0\n",
    "dataset = 'OPD'\n",
    "\n",
    "def kf_method(X, y, samples, participants):\n",
    "    \n",
    "    kf = list(KFold(n_splits=n_splits, shuffle=True).split(participants))\n",
    "    indices = []\n",
    "    for participant_train_index,participant_test_index in kf:\n",
    "        train_participants = participants.iloc[participant_train_index]['participant']\n",
    "        test_participants = participants.iloc[participant_test_index]['participant']\n",
    "        \n",
    "        cond1 = samples['participant'].isin(train_participants)\n",
    "        cond2 = samples['participant'].isin(test_participants)\n",
    "        \n",
    "        train_indices = samples.loc[cond1].index.tolist()\n",
    "        test_indices = samples.loc[cond2].index.tolist()\n",
    "        \n",
    "        indices.append((train_indices,test_indices))\n",
    "    return indices\n",
    "\n",
    "test_size = 1.0 - training_split\n",
    "def split_method(X, y, samples, participants):\n",
    "    split = train_test_split(participants,test_size=test_size)\n",
    "    \n",
    "    (train_participants,test_participants) = [c['participant'] for c in split]\n",
    "    cond1 = samples['participant'].isin(train_participants)\n",
    "    cond2 = samples['participant'].isin(test_participants)\n",
    "    \n",
    "    train_indices = samples.loc[cond1].index.tolist()\n",
    "    test_indices = samples.loc[cond2].index.tolist()\n",
    "    \n",
    "    return X[train_indices],X[test_indices],y[train_indices],y[test_indices]\n",
    "\n",
    "\n",
    "\n",
    "kf_results_OPD_1,split_results_OPD_1 = evaluation.evaluate(dataset, kf_method, split_method, samples, participants, to_numpy, global_settings, ozkan_settings, caliskan_settings, ulhaq_settings, ozkan_method, caliskan_method, ulhaq_method, repetitions, verbose_odds, seeds)\n",
    "\n",
    "display(kf_results_OPD_1)\n",
    "display(split_results_OPD_1)\n",
    "\n",
    "d = date.today().strftime(\"%d%B%y\")\n",
    "dl.pickleSave(kf_results_OPD_1,join('Results','kfold OPD digital fingerprinting removed ' + d + '.pickle'))\n",
    "dl.pickleSave(split_results_OPD_1,join('Results','traintest OPD digital fingerprinting removed' + d + '.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant independence and class balance (removing the digital fingerprinting and accuracy paradox effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=OPD_samples\n",
    "participants=OPD_participants\n",
    "\n",
    "ozkan_method=ozkan\n",
    "caliskan_method=caliskan\n",
    "ulhaq_method=ulhaq\n",
    "\n",
    "seeds=seeds\n",
    "n_splits=kfold_splits\n",
    "repetitions=repetitions\n",
    "verbose_odds=0\n",
    "dataset = 'OPD'\n",
    "\n",
    "def restrict_PD_samples(PD_samples,PD_participants):\n",
    "    restricted_PD_samples = pd.DataFrame()\n",
    "    #I know I need to remove 2/3 of all PD samples to match the sizes\n",
    "    for p in PD_participants['participant']: restricted_PD_samples = restricted_PD_samples.append(dh.df_retrieve(PD_samples,{'participant':p}).sample(n=2))\n",
    "    return restricted_PD_samples\n",
    "\n",
    "def kf_method(X, y, samples, participants):\n",
    "    control_samples = dh.df_retrieve(samples,{'status':0})\n",
    "    PD_samples = dh.df_retrieve(samples,{'status':1})\n",
    "    control_participants = dh.df_retrieve(participants,{'status':0})\n",
    "    PD_participants = dh.df_retrieve(participants,{'status':1})\n",
    "\n",
    "    restricted_PD_samples = restrict_PD_samples(PD_samples,PD_participants)\n",
    "    samples = restricted_PD_samples.append(control_samples)\n",
    "\n",
    "    kf_control = list(KFold(n_splits=n_splits, shuffle=True).split(control_participants))\n",
    "    kf_PD = list(KFold(n_splits=n_splits, shuffle=True).split(PD_participants))\n",
    "    \n",
    "    indices = []\n",
    "    for (control_train_index,control_test_index),(PD_train_index,PD_test_index) in zip(kf_control,kf_PD):\n",
    "        control_train_participants = control_participants.iloc[control_train_index]['participant']\n",
    "        control_test_participants = control_participants.iloc[control_test_index]['participant']\n",
    "        PD_train_participants = PD_participants.iloc[PD_train_index]['participant']\n",
    "        PD_test_participants = PD_participants.iloc[PD_test_index]['participant']\n",
    "        \n",
    "        cond1 = samples['participant'].isin(control_train_participants) | samples['participant'].isin(PD_train_participants)\n",
    "        cond2 = samples['participant'].isin(control_test_participants) | samples['participant'].isin(PD_test_participants)\n",
    "        \n",
    "        train_indices = samples.loc[cond1].index.tolist()\n",
    "        test_indices = samples.loc[cond2].index.tolist()\n",
    "        \n",
    "        indices.append((train_indices,test_indices))\n",
    "\n",
    "    return indices\n",
    "\n",
    "test_size = 1.0 - training_split\n",
    "def split_method(X, y, samples, participants):\n",
    "    control_samples = dh.df_retrieve(samples,{'status':0})\n",
    "    PD_samples = dh.df_retrieve(samples,{'status':1})\n",
    "    control_participants = dh.df_retrieve(participants,{'status':0})\n",
    "    PD_participants = dh.df_retrieve(participants,{'status':1})\n",
    "\n",
    "    restricted_PD_samples = restrict_PD_samples(PD_samples,PD_participants)\n",
    "    samples = restricted_PD_samples.append(control_samples)\n",
    "    \n",
    "    split_control = train_test_split(control_participants,test_size=test_size)\n",
    "    split_PD = train_test_split(PD_participants,test_size=test_size)\n",
    "    \n",
    "    (control_train_participants,control_test_participants) = [c['participant'] for c in split_control]\n",
    "    (PD_train_participants,PD_test_participants) = [c['participant'] for c in split_PD]\n",
    "\n",
    "    cond1 = samples['participant'].isin(control_train_participants) | samples['participant'].isin(PD_train_participants)\n",
    "    cond2 = samples['participant'].isin(control_test_participants) | samples['participant'].isin(PD_test_participants)\n",
    "\n",
    "    train_indices = samples.loc[cond1].index.tolist()\n",
    "    test_indices = samples.loc[cond2].index.tolist()\n",
    "    \n",
    "    return X[train_indices],X[test_indices],y[train_indices],y[test_indices]\n",
    "    #X_train, X_test, y_train, y_test\n",
    "\n",
    "kf_results_OPD_2,split_results_OPD_2 = evaluation.evaluate(dataset, kf_method, split_method, samples, participants, to_numpy, global_settings, ozkan_settings, caliskan_settings, ulhaq_settings, ozkan_method, caliskan_method, ulhaq_method, repetitions, verbose_odds, seeds)\n",
    "\n",
    "display(kf_results_OPD_2)\n",
    "display(split_results_OPD_2)\n",
    "\n",
    "d = date.today().strftime(\"%d%B%y\")\n",
    "dl.pickleSave(kf_results_OPD_2,join('Results','kfold OPD digital fingerprinting and accuracy paradox removed ' + d + '.pickle'))\n",
    "dl.pickleSave(split_results_OPD_2,join('Results','traintest OPD digital fingerprinting and accuracy paradox removed' + d + '.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All effects removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_results_OPD_3 = dl.pickleLoad(join('Results','repeated_kfold with OPD modified 25October21.pickle'))\n",
    "split_results_OPD_3 = dl.pickleLoad(join('Results','repeated_traintest with OPD modified25October21.pickle'))\n",
    "\n",
    "rh.print_results(kf_results_OPD_3,top=1)\n",
    "rh.print_results(split_results_OPD_3,top=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the mPower data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 5\n",
    "training_split = 0.7\n",
    "repetitions = 10\n",
    "\n",
    "###################### Seeds ######################\n",
    "seeds = list(range(repetitions))\n",
    "###################################################\n",
    "\n",
    "\n",
    "######################## to_numpy method ########################\n",
    "\n",
    "def to_numpy(mPower_samples):\n",
    "\n",
    "    features = ['Mean pitch (Hz)','Minimum pitch (Hz)','Maximum pitch (Hz)','Jitter (local) (%)','Jitter (local, absolute)','Jitter (rap) (%)','Jitter (ppq5) (%)','Jitter (ddp) (%)',\n",
    "                'Shimmer (local) (%)','Shimmer (local, dB) (dB)','Shimmer (apq3) (%)','Shimmer (apq5) (%)','Shimmer (apq11) (%)','Shimmer (dda) (%)',\n",
    "                'Mean noise-to-harmonics ratio','Mean harmonics-to-noise ratio (dB)','spread1 (negative entropy of F0)','spread2 (standard error of F0)','PPE','DFA','RPDE','d2']\n",
    "    \n",
    "    X = mPower_samples[features].to_numpy()\n",
    "    y = (mPower_samples['professional-diagnosis']*1).to_numpy(dtype='int64')\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "#################################################################\n",
    "\n",
    "\n",
    "#The model we will be using for Ozkan is \"ozkan PCA_16 k_11 StandardScaler X only\"\n",
    "#This was the best model for both 10fold CV and 70/30 split\n",
    "\n",
    "#The model we will be using for Caliskan is \"caliskan ReLU Sigmoid latent:6, epochs:50, lr:0.0030 StandardScaler X only\"\n",
    "#This was the best model for both 10fold CV and 70/30 split\n",
    "\n",
    "#The model we will be using for Ulhaq is \"ulhaq rbf gamma:scale C:10 num_features:20 StandardScaler X only\"\n",
    "#This was the 2nd best model for 10fold CV and best for 70/30 split\n",
    "\n",
    "\n",
    "######################## Global settings ########################\n",
    "preprocessors = [StandardScaler]\n",
    "preprocessing_methods = ['X only']\n",
    "global_settings = list(itertools.product(preprocessors,preprocessing_methods))\n",
    "##################################################################\n",
    "\n",
    "######################## Ozkan settings ########################\n",
    "components = [16]\n",
    "ks = [11]\n",
    "ozkan_settings = list(itertools.product(components,ks))\n",
    "################################################################\n",
    "\n",
    "######################## Caliskan settings ########################\n",
    "lrses = [[0.003]*4,]\n",
    "epochses = [[50]*4,]\n",
    "rhoses = [[0.15,0.25],]\n",
    "lamses = [[0.03,0.03],]\n",
    "Bses = [[2,2],]\n",
    "activationses = [[nn.ReLU,nn.Sigmoid],]\n",
    "latent_sizes = [6,]\n",
    "caliskan_settings = list(itertools.product(lrses,epochses,rhoses,lamses,Bses,activationses,latent_sizes))\n",
    "###################################################################\n",
    "\n",
    "######################## Ul-Haq settings ########################\n",
    "kernels = ['rbf']\n",
    "gammas = ['scale']\n",
    "Cs = [10]\n",
    "num_featureses = [20] #best at 10 in paper\n",
    "ulhaq_settings = list(itertools.product(kernels,gammas,Cs,num_featureses))\n",
    "#################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First the unmodified results (no inflatory effects removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_results_mPower_0 = dl.pickleLoad(join('Results','repeated_kfold with mPower unmodified 24January22.pickle'))\n",
    "split_results_mPower_0 = dl.pickleLoad(join('Results','repeated_traintest with mPower unmodified24January22.pickle'))\n",
    "\n",
    "rh.print_results(kf_results_mPower_0,top=2)\n",
    "rh.print_results(split_results_mPower_0,top=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant independence (removing the digital fingerprinting effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples=mPower_samples\n",
    "participants=mPower_participants\n",
    "\n",
    "ozkan_method=ozkan\n",
    "caliskan_method=caliskan\n",
    "ulhaq_method=ulhaq\n",
    "\n",
    "seeds=seeds\n",
    "n_splits=kfold_splits\n",
    "repetitions=repetitions\n",
    "verbose_odds=0\n",
    "dataset = 'mPower'\n",
    "\n",
    "def kf_method(X, y, samples, participants):\n",
    "    kf = list(KFold(n_splits=n_splits, shuffle=True).split(participants))\n",
    "    indices = []\n",
    "    for participant_train_index,participant_test_index in kf:\n",
    "        train_participants = participants.iloc[participant_train_index]['healthCode']\n",
    "        test_participants = participants.iloc[participant_test_index]['healthCode']\n",
    "        \n",
    "        cond1 = samples['healthCode'].isin(train_participants)\n",
    "        cond2 = samples['healthCode'].isin(test_participants)\n",
    "        \n",
    "        train_indices = samples.loc[cond1].index.tolist()\n",
    "        test_indices = samples.loc[cond2].index.tolist()\n",
    "        \n",
    "        indices.append((train_indices,test_indices))\n",
    "    return indices\n",
    "\n",
    "test_size = 1.0 - training_split\n",
    "def split_method(X, y, samples, participants):\n",
    "    split = train_test_split(participants,test_size=test_size)\n",
    "    \n",
    "    (train_participants,test_participants) = [c['healthCode'] for c in split]\n",
    "    cond1 = samples['healthCode'].isin(train_participants)\n",
    "    cond2 = samples['healthCode'].isin(test_participants)\n",
    "    \n",
    "    train_indices = samples.loc[cond1].index.tolist()\n",
    "    test_indices = samples.loc[cond2].index.tolist()\n",
    "    \n",
    "    return X[train_indices],X[test_indices],y[train_indices],y[test_indices]\n",
    "\n",
    "\n",
    "kf_results_mPower_1,split_results_mPower_1 = evaluation.evaluate(dataset, kf_method, split_method, samples, participants, to_numpy, global_settings, ozkan_settings, caliskan_settings, ulhaq_settings, ozkan_method, caliskan_method, ulhaq_method, repetitions, verbose_odds, seeds)\n",
    "\n",
    "display(kf_results_mPower_1)\n",
    "display(split_results_mPower_1)\n",
    "\n",
    "d = date.today().strftime(\"%d%B%y\")\n",
    "dl.pickleSave(kf_results_mPower_1,join('Results','kfold mPower digital fingerprinting removed ' + d + '.pickle'))\n",
    "dl.pickleSave(split_results_mPower_1,join('Results','traintest mPower digital fingerprinting removed' + d + '.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participant independence and class balance (removing the digital fingerprinting and accuracy paradox effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mPower_modifications as mm\n",
    "import numpy as np\n",
    "samples=mPower_samples\n",
    "participants=mPower_participants\n",
    "\n",
    "ozkan_method=ozkan\n",
    "caliskan_method=caliskan\n",
    "ulhaq_method=ulhaq\n",
    "\n",
    "seeds=seeds\n",
    "n_splits=kfold_splits\n",
    "repetitions=repetitions\n",
    "verbose_odds=0\n",
    "dataset = 'mPower'\n",
    "\n",
    "def kf_method(X, y, samples, participants):\n",
    "    \n",
    "    PD_samples = dh.df_retrieve(samples,{'professional-diagnosis':True})\n",
    "    control_samples = dh.df_retrieve(samples,{'professional-diagnosis':False})\n",
    "    \n",
    "    target = min(len(PD_samples),len(control_samples))\n",
    "    PD_samples = PD_samples.sample(n=target)\n",
    "    control_samples = control_samples.sample(n=target)\n",
    "    \n",
    "    PD_folds = mm.create_k_folds(PD_samples,k=n_splits)\n",
    "    control_folds = mm.create_k_folds(control_samples,k=n_splits)\n",
    "\n",
    "    PD_mapping = {}\n",
    "    control_mapping = {}\n",
    "    for fold in range(n_splits):\n",
    "        PD_mapping.update(dict.fromkeys(PD_folds.loc[fold,'participants'],fold))\n",
    "        control_mapping.update(dict.fromkeys(control_folds.loc[fold,'participants'],fold))\n",
    "    \n",
    "    PD_samples['fold'] = PD_samples['healthCode'].map(PD_mapping)\n",
    "    control_samples['fold'] = control_samples['healthCode'].map(control_mapping)\n",
    "    mapping = {key:val for key,val in list(zip(PD_samples['recordId'],PD_samples['fold']))}\n",
    "    mapping.update({key:val for key,val in list(zip(control_samples['recordId'],control_samples['fold']))})\n",
    "        \n",
    "    samples['fold'] = samples['recordId'].map(mapping)\n",
    "        \n",
    "    fold_indices = [np.where(samples['fold']==fold)[0] for fold in range(n_splits)]\n",
    "\n",
    "    folds = []\n",
    "    \n",
    "    iter_i = np.random.permutation(len(fold_indices))\n",
    "    for i in iter_i:\n",
    "        test_indices = fold_indices[i]\n",
    "        train_indices = [fold_indices[j] for j in iter_i[iter_i != i]]\n",
    "        train_indices = list(itertools.chain.from_iterable(train_indices))\n",
    "        folds.append((train_indices,test_indices))\n",
    "    \n",
    "\n",
    "    return folds\n",
    "\n",
    "test_size = 1.0 - training_split\n",
    "def split_method(X, y, samples, participants):\n",
    "    \n",
    "    PD_samples = dh.df_retrieve(samples,{'professional-diagnosis':True})\n",
    "    control_samples = dh.df_retrieve(samples,{'professional-diagnosis':False})\n",
    "    \n",
    "    target = min(len(PD_samples),len(control_samples))\n",
    "    PD_samples = PD_samples.sample(n=target)\n",
    "    control_samples = control_samples.sample(n=target)\n",
    "    \n",
    "    PD_folds = mm.create_k_folds(PD_samples,k=10)\n",
    "    control_folds = mm.create_k_folds(control_samples,k=10)\n",
    "    \n",
    "\n",
    "    PD_mapping = {}\n",
    "    control_mapping = {}\n",
    "    for fold in range(10):\n",
    "        PD_mapping.update(dict.fromkeys(PD_folds.loc[fold,'participants'],fold))\n",
    "        control_mapping.update(dict.fromkeys(control_folds.loc[fold,'participants'],fold))\n",
    "    \n",
    "    PD_samples['fold'] = PD_samples['healthCode'].map(PD_mapping)\n",
    "    control_samples['fold'] = control_samples['healthCode'].map(control_mapping)\n",
    "    mapping = {key:val for key,val in list(zip(PD_samples['recordId'],PD_samples['fold']))}\n",
    "    mapping.update({key:val for key,val in list(zip(control_samples['recordId'],control_samples['fold']))})\n",
    "        \n",
    "    samples['fold'] = samples['recordId'].map(mapping)\n",
    "        \n",
    "    fold_indices = [np.where(samples['fold']==fold)[0] for fold in range(10)]\n",
    "    \n",
    "    #combine 3 random folds\n",
    "    test_indices = []\n",
    "    for _ in range(3):\n",
    "        i = np.random.randint(0,len(fold_indices))\n",
    "        test_indices.append(fold_indices[i])\n",
    "        fold_indices.pop(i)\n",
    "        \n",
    "    train_indices = [idx for sublist in fold_indices for idx in sublist]\n",
    "    test_indices = [idx for sublist in test_indices for idx in sublist]\n",
    "    \n",
    "    return X[train_indices],X[test_indices],y[train_indices],y[test_indices]\n",
    "\n",
    "\n",
    "\n",
    "kf_results_mPower_2,split_results_mPower_2 = evaluation.evaluate(dataset, kf_method, split_method, samples, participants, to_numpy, global_settings, ozkan_settings, caliskan_settings, ulhaq_settings, ozkan_method, caliskan_method, ulhaq_method, repetitions, verbose_odds, seeds)\n",
    "\n",
    "display(kf_results_mPower_2)\n",
    "display(split_results_mPower_2)\n",
    "\n",
    "d = date.today().strftime(\"%d%B%y\")\n",
    "dl.pickleSave(kf_results_mPower_2,join('Results','kfold mPower digital fingerprinting and accuracy paradox removed ' + d + '.pickle'))\n",
    "dl.pickleSave(split_results_mPower_2,join('Results','traintest mPower digital fingerprinting and accuracy paradox removed' + d + '.pickle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With all effects removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_results_mPower_3 = dl.pickleLoad(join('Results','repeated_kfold with mPower modified 25January22.pickle'))\n",
    "split_results_mPower_3 = dl.pickleLoad(join('Results','repeated_traintest with mPower modified25January22.pickle'))\n",
    "\n",
    "rh.print_results(kf_results_mPower_3,top=1)\n",
    "rh.print_results(split_results_mPower_3,top=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PD Voice Replication 2)\n",
   "language": "python",
   "name": "pd_voice_replication_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
